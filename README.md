# Hadoop-Small-Files
Impact of Small files on Hadoop Performance.
A small file is one which is significantly smaller than the HDFS block size (default 64MB/128MB). If were storing small files, then we probably have lots of blocks, otherwise we wouldn’t turn to Hadoop, and the problem is that HDFS can’t handle lots of files. As a general rule, each file, directory, and block in HDFS is represented as an object in the namenode's memory, which typically takes up 150 bytes. Thus, if 10 million files were used, each requiring a block, 3 gigabytes of memory would be required. With existing gear, scaling up considerably beyond this point is problematic. A billion files are definitely not doable.

The paper focused on the two metrics of the Hadoop namenode fsimage loading time and process time to ingest data into the datanode. Four different number of files were loaded on four single node clusters. The files were of different sizes with the first case having 1067 1MB files and case 2 had 267 4MB files, Case 3 had 7 154MB sized files and Case 4 had 1Gig 1 file. All the cases were of the same size 1Gig. 

The experiments established a positive relationship between the number of files and the time taken by the namenode fsimage to load when a Hadoop cluster is started. The experiment also established it was faster to ingest a single 1 gig file as compared to 1gig of 1000 1MB files. The paper went on to discuss possible sources of small files and established the sequencing file approach to deal with small files in Hadoop. For future research the paper noted the need to use a multi-cluster and also to observe live streaming data processing and measure other metrics that affect the namenode when dealing with small files.
